{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Estructura:*\n",
    "\n",
    "## Lectura del historico del mes:\n",
    "\n",
    "El proceso comienza cargando todos los ficheros de alarmas del mes en curso. Sí hay ficheros los lee y guarda los msisdn en un diccionario como clave. si no existe ningun fichero se genera un dataframe vacio con la misma estructura que en el historico. El sentido de cargar el pasado antes de analizar el presente es porque no se quiere reportar msisdn en un mes que ya se hayan reportado.\n",
    "\n",
    "## Lectura Fichero_detalle_actual y General Zonas\n",
    "\n",
    "Una vez se lee el fichero_detalle_actual se le aplica un left join en el sentido (Fichero_detalle_actual <- General Zonas) con el fin de aportarle al msisdn la información del operador con el previo tratamiento y aplicación de lógicas al general zonas. Tras esto aparece una posible casuística y como es un proceso sensible se pregunta a la columna \"COD_CLIENTE_PADRE\" si tiene NaN, ya que han podido aparecer tras el cruce. (el operador se metería a mano tras consultar el msisdn en ATENEA)\n",
    "\n",
    "Con la finalización de este punto y el posterior agrupado por ['msisdn','NAME','COD_CLIENTE_PADRE','recordType'] llegamos al siguiente escenario planteado en el apartado anterior:\n",
    "\n",
    "-> df_historico *empty*: en el dataframe actual se genera un columna \"FLAG\" donde todos las filas son False.(False será la condición necesaria para que un msisdn sea candidato a alarma)\n",
    "            \n",
    "-> df_historico *not empty*: se genera un diccionario con los msisdn como clave y se itera sobre la columna columna \"msisdn\" del dataframe actual para preguntarle si está en el diccionario   historico, de tal manera de que si lo encuentra en el diccionario devuelve un True (msisdn que no es candidato a enviar en alarma) y sino False.\n",
    "\n",
    "Con la categorización de lo que es candidato a enviar se realiza un filtro donde se seleciona todo lo que sea False.\n",
    "\n",
    "El siguiente punto sería añadir los directorios realizando un LEFT JOIN sobre el operador para incorporar los emails. Y de la misma manera que se realizó anteriormente una correción sobre el merge, aquí lo volvemos a repetir en caso de que haya alguno campo de la columna \"MAIL\" como NaN.\n",
    "\n",
    "## Función enviar_correos:\n",
    "\n",
    "En esta función se crea la configuración al servidor SMTP, incorpora las credencias del email a traves de un fichero yaml y se aplican una seríe de lógicas para que configure el campo \"FROM\" y \"SUBJECT\" en función del operador. Además, hay un control mediante los except de los errores que puedan ocurrir durante el envio. En este punto se guarda en una lista todos los operadores que han sufrido un error en los envios de tal forma que cuando se termine de enviar todos los correos se pueda analizar y reenviar los operdores con error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libreria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import _get_general_zonas, get_email\n",
    "from modulodirectorio import get_path\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import quopri\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "fecha_actual = datetime.now()\n",
    "año = fecha_actual.year\n",
    "mes = fecha_actual.month\n",
    "dia = fecha_actual.day\n",
    "date_str = str(año)+'_'+str(mes)+'_'+str(dia)\n",
    "\n",
    "mes_revision = '202307'  # TODO considerar obtener el mes a partir de la fecha actual, o si acaso, desde el fichero de detalle (para lidiar con cambios de mes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dir reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_detalle = get_path('dir_detalle')\n",
    "dir_historico_procesed = get_path('dir_historico_procesed')\n",
    "directorio_base = get_path('directorio_base')\n",
    "directorio_crudo = get_path('directorio_crudo')\n",
    "directorio_email = get_path('directorio_email')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura retarificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lectura_retarificaciones(mes, base_dir):\n",
    "    file_key = str(mes_revision)+' retarificación'\n",
    "\n",
    "    for nameCSV in glob.glob(os.path.join(base_dir ,\"*.xlsx\")):\n",
    "        file_name=str(nameCSV)\n",
    "        if file_key in file_name:\n",
    "            retarificaciones = pd.read_excel(file_name)\n",
    "            \n",
    "    retarificaciones['msisdn'] = retarificaciones['msisdn'].astype(str)\n",
    "    retarificaciones['variacion precio'] = retarificaciones['importe ocs'] - retarificaciones['importe correcto']\n",
    "    retarificaciones = retarificaciones.groupby(['msisdn']).sum().reset_index()\n",
    "    retarificaciones = retarificaciones[['msisdn','variacion precio']]\n",
    "    retarificaciones = retarificaciones[retarificaciones['variacion precio'] > 0] # Nos quedamos solo con las aminoraciones de importe\n",
    "    return retarificaciones\n",
    "\n",
    "\n",
    "def check_retarificaciones(mes, base_dir, df):\n",
    "    # primero averiguamos si se han hecho las retarificaciones\n",
    "    while True:\n",
    "        retarificaciones_realizadas = input('¿Se han realizado retarificaciones en el momento de envío de las alarmas? (s/n): ')\n",
    "\n",
    "        if retarificaciones_realizadas.lower() != 's' and retarificaciones_realizadas.lower() != 'n':\n",
    "            print('Respuesta inválida. Por favor, ingresa \"s\" o \"n\".')\n",
    "        else: break\n",
    "    \n",
    "    # si no se han hecho, hemos terminado\n",
    "    if retarificaciones_realizadas.lower() == 'n':\n",
    "        df['FLAG'] = False\n",
    "        return df\n",
    "\n",
    "    # en caso afirmativo, comprobamos los importes\n",
    "    df = df.merge(lectura_retarificaciones(mes_revision, base_dir), how = 'left', on = 'msisdn')\n",
    "    df['variacion precio'] = df['variacion precio'].fillna(0)\n",
    "    df['Importe Acumulado'] = df['Importe Acumulado'].astype(float)\n",
    "    df['Porcentaje retarificacion'] = (df['variacion precio']/df['Importe Acumulado'])*100\n",
    "    df['FLAG'] = df['Porcentaje retarificacion'].map(lambda x: x >= 75)\n",
    "    \n",
    "    print('Los msisdn con alguna retarifacion son:')\n",
    "    display(df[df['Porcentaje retarificacion'] > 0])\n",
    "    print('')\n",
    "    print('Aquellos con retarificaciones significativas son')\n",
    "    display(df[df['FLAG']])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura dir email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contactos = pd.read_csv(directorio_email, encoding='ansi', sep=';')\n",
    "df_contactos['Cod Operador'] = df_contactos['Cod Operador'].astype(str)\n",
    "\n",
    "if df_contactos.isnull().values.any(): raise ValueError('Valores faltantes en el fichero de contactos.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura del historico del mes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos todos los históricos que contienen yyyymm, para unificarlos en uno\n",
    "\n",
    "files = []\n",
    "\n",
    "for nameCSV in glob.glob(os.path.join(dir_historico_procesed, '*.csv')):\n",
    "    name = str(nameCSV)\n",
    "    if mes_revision in name and os.path.isfile(name):\n",
    "        name = pd.read_csv(name, delimiter=';', encoding='ansi')\n",
    "        files.append(name)\n",
    "\n",
    "if files:\n",
    "    df_historico = pd.concat(files)\n",
    "else:  # la lista está vacía, esto ocurre en mes nuevo\n",
    "    df_historico = pd.DataFrame(\n",
    "        columns=['msisdn', 'Cod Operador', 'Tipo de Evento', 'Nº Eventos', 'Suma Minutos', 'Importe en el mes', 'Importe Acumulado', 'Suma MB','billing period']) \n",
    "\n",
    "msisdn_previos = df_historico['msisdn'].unique().tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura Fichero_detalle_actual y General Zonas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FICHERO DETALLE-------------------------------------------------------------------------\n",
    "\n",
    "# primero, verificamos que sólo exista un fichero en el directorio de detalle\n",
    "file_list = glob.glob(os.path.join(dir_detalle, '*.txt'))\n",
    "if not file_list: raise ValueError(f'No hay fichero en el directorio de detalle: {dir_detalle}')\n",
    "if len(file_list) > 1: raise ValueError(f'Existen múltiples ficheros en el directorio de detalle: {dir_detalle}')\n",
    "\n",
    "filepath = file_list[0]\n",
    "filename = os.path.basename(filepath)\n",
    "\n",
    "# ahora, verificamos que el nombre del fichero tenga sentido\n",
    "if not bool(re.match(r'^{}(\\d{{2}}).*'.format(mes_revision), filename)):\n",
    "    raise ValueError('El fichero no tiene un nombre válido')\n",
    "\n",
    "# leemos el fichero y lo movemos al histórico de crudos\n",
    "df_detalle = pd.read_csv(filepath, delimiter=';', decimal = ',')\n",
    "shutil.move(filepath, directorio_crudo)\n",
    "\n",
    "\n",
    "# GENERAL ZONAS---------------------------------------------------------------------------\n",
    "zonas = _get_general_zonas()  # TODO considerar utilizar simplemente el más actualizado, así no se requiere el cambio del nombre\n",
    "g_zonas = pd.read_csv(zonas,sep=';')\n",
    "g_zonas = g_zonas.sort_values('FEC_ALTA_ABONADO', ascending=True)\n",
    "g_zonas = g_zonas.drop_duplicates(subset='ES_MOVIL_FIXED', keep='first')\n",
    "g_zonas = g_zonas[['ES_MOVIL_FIXED','COD_CLIENTE_PADRE','OPERADOR','NAME']]\n",
    "g_zonas = g_zonas.rename(columns={'ES_MOVIL_FIXED': 'msisdn', 'COD_CLIENTE_PADRE': 'COD OPERADOR'})\n",
    "g_zonas['COD OPERADOR'] = g_zonas['COD OPERADOR'].astype(str)\n",
    "\n",
    "\n",
    "# LEFT JOIN (Fichero_detalle_actual <- General Zonas)-------------------------------------\n",
    "df_detalle_clean = df_detalle.merge(g_zonas, how = 'left', on='msisdn')\n",
    "\n",
    "\n",
    "# COMPROBACION DE LOS NULOS --------------------------------------------------------------\n",
    "\n",
    "# vamos a iterar sobre los nulos, controlando tanto sus posiciones en el df, como el progreso en la entrada de datos\n",
    "null_positions = [i for i, value in enumerate(df_detalle_clean['COD OPERADOR']) if pd.isnull(value)]\n",
    "num_null_values = len(null_positions)\n",
    "\n",
    "for i, pos in enumerate(null_positions, start=1):\n",
    "    msisdn = df_detalle_clean.at[pos, 'msisdn']\n",
    "    correct_value = str(int(input(f'Ingrese el código del operador en ATENEA para el MSISDN {msisdn} ({i}/{num_null_values})')))\n",
    "    df_detalle_clean.at[pos, 'COD OPERADOR'] = correct_value\n",
    "\n",
    "# realizamos los agrupados, y preparamos el formato\n",
    "\n",
    "df_detalle_clean = df_detalle_clean.groupby(\n",
    "    by=['msisdn', 'NAME','COD OPERADOR','recordType','priceAcu']).agg(\n",
    "        {'units': sum, 'minutes': sum, 'price': sum, 'mb': sum}).reset_index()\n",
    "\n",
    "df_detalle_clean = df_detalle_clean.rename(\n",
    "    columns = {'COD OPERADOR': 'Cod Operador',\n",
    "               'NAME': 'Operador',\n",
    "               'priceAcu': 'Importe Acumulado',\n",
    "               'recordType': 'Tipo de Evento',\n",
    "               'units': 'Nº Eventos',\n",
    "               'minutes': 'Suma Minutos',\n",
    "               'mb': 'Suma MB',\n",
    "               'price': 'Importe en el mes'})\n",
    "\n",
    "df_detalle_clean['Periodo'] = mes_revision\n",
    "df_detalle_clean['msisdn'] = df_detalle_clean['msisdn'].astype(str)\n",
    "\n",
    "# como los inicios del mes siempre va haber un dataframe vacio, todo lo que haya en el fichero es envio. Así nos evitamos errores en el merge.\n",
    "if not df_historico.empty: df_detalle_clean['FLAG'] = df_detalle_clean['msisdn'].map(lambda x: x in msisdn_previos)\n",
    "else: df_detalle_clean['FLAG'] = False # como está vacio se envia todo\n",
    "\n",
    "df_detalle_clean = check_retarificaciones(mes_revision, base_dir, df_detalle_clean)\n",
    "\n",
    "# nos quedamos con aquellos sin retarificaciones masivas\n",
    "df_detalle_clean = df_detalle_clean[~df_detalle_clean['FLAG']]\n",
    "df_detalle_clean.to_csv(get_path('dir_historico_procesed') / (filename[:-4]+'.csv'), index = False, sep = ';', encoding='ansi')\n",
    "\n",
    "# para cruzar los contactos, primero trabajamos a nivel operador\n",
    "df_mail = pd.DataFrame(df_detalle_clean['Cod Operador'].drop_duplicates())\n",
    "df_mail = df_mail.merge(df_contactos, how = 'left', on ='Cod Operador')\n",
    "\n",
    "# añadimos aquellos que falten\n",
    "null_positions = [i for i, value in enumerate(df_mail['MAIL']) if pd.isnull(value)]\n",
    "num_null_values = len(null_positions)\n",
    "\n",
    "for i, pos in enumerate(null_positions, start=1):\n",
    "    cod = df_mail.at[pos, 'Cod Operador']\n",
    "    correct_value = str(input(f'Ingrese el email de contacto para el operador {cod} ({i}/{num_null_values})'))\n",
    "    # añadimos los nuevos contactos en el df de contactos\n",
    "    new_row = {'Cod Operador': cod, 'MAIL': correct_value}\n",
    "    df_contactos = df_contactos.append(new_row, ignore_index=True)\n",
    "\n",
    "# guardamos el fichero de contactos\n",
    "df_contactos.to_csv(directorio_email, encoding='ansi', sep=';', index=False)\n",
    "\n",
    "# cruzamos a nivel MSISDN\n",
    "df_final = df_detalle_clean.merge(df_contactos, how='left', on='Cod Operador')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enviar_correo(destinatario, archivo_adjunto, operador, date_str, email, passw):\n",
    "\n",
    "    destinatario = destinatario.split(' ')\n",
    "    \n",
    "    if len(destinatario) <= 1 and any('' == email for email in destinatario):\n",
    "        print(destinatario)\n",
    "        return False\n",
    "    \n",
    "    destinatario.append('ejemplo@:).es')\n",
    "    \n",
    "    # Configurar el servidor SMTP y el mensaje\n",
    "    smtp_server = 'smtp-mail.outlook.com'\n",
    "    smtp_port = 587\n",
    "    smtp_username = email\n",
    "    smtp_password = passw\n",
    "    body = '''CUERPO DEL CORREO\n",
    "'''\n",
    "\n",
    "    # Crear el objeto del correo electrónico\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = 'ejemplo@:).es'\n",
    "    recipients = ', '.join(destinatario)\n",
    "\n",
    "    # gestionar caso recipiente ''\n",
    "    msg['To'] = recipients\n",
    "\n",
    "    msg['Subject'] = 'Alarma de consumo superior a 49 € - ' + operador\n",
    "    msg.attach(MIMEText(body, 'plain'))\n",
    "    # Adjuntar el archivo al correo electrónico\n",
    "    with open(archivo_adjunto, 'rb') as file:\n",
    "        attachment = MIMEBase('application', 'octet-stream')\n",
    "        attachment.set_payload(file.read())\n",
    "        encoders.encode_base64(attachment)\n",
    "        attachment.add_header('Content-Disposition', 'attachment', filename= date_str + '_' +operador + '.xlsx')\n",
    "        msg.attach(attachment)\n",
    "\n",
    "    # Enviar el correo electrónico\n",
    "    with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
    "        server.starttls()\n",
    "        server.login(smtp_username, smtp_password)\n",
    "        server.send_message(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email, passw = get_email()\n",
    "\n",
    "operadores = df_final['Cod Operador'].unique()\n",
    "\n",
    "error_files = []\n",
    "\n",
    "c = 0\n",
    "c_e = 0\n",
    "\n",
    "for operador in operadores:\n",
    "    # Crear DataFrame para el operador actual\n",
    "    df_operador = df_final[df_final['Cod Operador'] == operador].copy()\n",
    "    destinatario = df_operador['MAIL'].iloc[0]\n",
    "\n",
    "    operador_name = df_operador['Operador'].iloc[0]\n",
    "    df_operador = df_operador[\n",
    "        ['msisdn', 'Cod Operador', 'Tipo de Evento', 'Nº Eventos', 'Suma Minutos', 'Importe en el mes', 'Importe Acumulado', 'Suma MB']]\n",
    "    directorio_operador = os.path.join(directorio_base, operador)\n",
    "    \n",
    "    if not os.path.exists(directorio_operador):\n",
    "        os.makedirs(directorio_operador)\n",
    "    \n",
    "    archivo_csv = os.path.join(directorio_base, operador)+'/'+date_str+'.xlsx'\n",
    "    df_operador.to_excel(os.path.join(directorio_base, operador)+'/'+date_str+'.xlsx', index=False, encoding= 'ansi')\n",
    "    \n",
    "    try:\n",
    "        enviar_correo(destinatario, archivo_csv, operador_name, date_str, email, passw)\n",
    "        c+=1\n",
    "        print(f'Correo {c} enviado satisfactoriamente -> {operador}')\n",
    "        \n",
    "    except:  \n",
    "        c_e+=1  \n",
    "        error_files.append(operador)\n",
    "        print(f'Correo {c_e} error en el envio -> {operador}')\n",
    "       \n",
    "print('Errores:')\n",
    "print(error_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reenvio de los correos con la rectificación del email \n",
    "\n",
    "for operador in error_files:\n",
    "    \n",
    "    # Crear DataFrame para el operador actual\n",
    "    df_operador = prueba[prueba['Cod Operador'] == operador].copy()\n",
    "    destinatario = df_operador['MAIL'].iloc[0]\n",
    "    operador_name = df_operador['Operador'].iloc[0]\n",
    "    df_operador = df_operador[['msisdn', 'Cod Operador', 'Tipo de Evento', 'Nº Eventos', 'Suma Minutos', 'Importe en el mes', 'Importe Acumulado', 'Suma MB']]\n",
    "    directorio_operador = os.path.join(directorio_base, operador)\n",
    "    \n",
    "    if not os.path.exists(directorio_operador):\n",
    "        os.makedirs(directorio_operador)\n",
    "    \n",
    "    print(f\"El directorio '{directorio_operador}' ya existe.\")\n",
    "    archivo_csv = os.path.join(directorio_base, operador)+'/'+date_str+'.xlsx'\n",
    "    \n",
    "    df_operador.to_excel(os.path.join(directorio_base, operador)+'/'+date_str+'.xlsx', index=False, encoding= 'ansi')\n",
    "    if not enviar_correo(destinatario, archivo_csv, operador_name, date_str):\n",
    "        error_files.append(operador)\n",
    "        \n",
    "       \n",
    "\n",
    "print(error_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1.0conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
